{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import en_core_web_lg\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"diagram.png\" style=\"height: 50px; width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starwars_text = 'Darth Vader, also known by his birth name Anakin Skywalker, is a fictional character in the Star Wars franchise. Darth Vader appears in the original film trilogy as a pivotal antagonist whose actions drive the plot, while his past as Anakin Skywalker and the story of his corruption are central to the narrative of the prequel trilogy. The character was created by George Lucas and has been portrayed by numerous actors. His appearances span the first six Star Wars films, as well as Rogue One, and his character is heavily referenced in Star Wars: The Force Awakens. He is also an important character in the Star Wars expanded universe of television series, video games, novels, literature and comic books. Originally a Jedi prophesied to bring balance to the Force, he falls to the dark side of the Force and serves the evil Galactic Empire at the right hand of his Sith master, Emperor Palpatine (also known as Darth Sidious).'\n",
    "starwars_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: 6 sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpaCy NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_lg.load()\n",
    "doc = nlp(starwars_text)\n",
    "\n",
    "ner_dict = {}\n",
    "for x in doc.ents:\n",
    "    ner_dict[x.text] = x.label_\n",
    "ner_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Does not perform well at all compared to SpaCy NER. Able to recognize PERSONs but not in partial fragments. Not able to recognize entities other than LOCATION or PERSON such as WORK_OF_ART or DATE, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NLTK\n",
    "# # sentences = nltk.sent_tokenize(starwars_text)\n",
    "\n",
    "# spaCy\n",
    "run_this=0\n",
    "if run_this==1:\n",
    "    nlp = spacy.lang.en.English()\n",
    "    nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "    doc = nlp(starwars_text)\n",
    "    sentences = [sent.string.strip() for sent in doc.sents]\n",
    "\n",
    "    ner_tagger = nltk.tag.StanfordNERTagger(\"./stanford-ner-2018-10-16/classifiers/english.all.3class.distsim.crf.ser.gz\", \"./stanford-ner-2018-10-16/stanford-ner.jar\")\n",
    "\n",
    "    ner_dict = {}\n",
    "    results = []\n",
    "\n",
    "    nlp = spacy.lang.en.English()\n",
    "    tokenizer = spacy.tokenizer.Tokenizer(nlp.vocab)\n",
    "    for sent in sentences:\n",
    "        words = [token.orth_ for token in tokenizer(sent)]\n",
    "        print(words)\n",
    "        tagged = ner_tagger.tag(words)\n",
    "        results += tagged\n",
    "\n",
    "    for res in results:\n",
    "        ner_dict[res[0]] = res[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coreference Resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> fixed root permission error: https://github.com/Lynten/stanford-corenlp/issues/26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate coreferences and dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = StanfordCoreNLP(\"./stanford-corenlp-4.2.0\", quiet=False)\n",
    "annotated = nlp.annotate(starwars_text, properties={'annotators': 'coref', 'pipelineLanguage': 'en'})\n",
    "result = json.loads(annotated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resolve coreferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corefs = result['corefs']\n",
    "print(\"Coreferences found: \",len(corefs))\n",
    "print(\"Named entities: \" , ner_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_coref_with = []\n",
    "sentence_wise_replacements = defaultdict(list) \n",
    "sentence_wise_replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(starwars_text)\n",
    "# nlp = spacy.lang.en.English()\n",
    "# nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "# doc = nlp(starwars_text)\n",
    "# sentences = [sent.string.strip() for sent in doc.sents]\n",
    "\n",
    "print('Number of sentences: ', len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Here, nltk sentence tokenizer is more accurate than spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,coreferences in enumerate(corefs.values()):\n",
    "    replace_with = coreferences[0]\n",
    "    for reference in coreferences:\n",
    "        if reference[\"text\"] in ner_dict.keys() or reference[\"text\"][reference[\"headIndex\"]-reference[\"startIndex\"]] in ner_dict.keys():\n",
    "            replace_with = reference\n",
    "        sentence_wise_replacements[reference[\"sentNum\"]-1].append((reference,index))\n",
    "    replace_coref_with.append(replace_with[\"text\"])  \n",
    "\n",
    "sentence_wise_replacements[0].sort(key=lambda tup: tup[0][\"startIndex\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pronoun Replacement with Named Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.lang.en.English()\n",
    "# tokenizer = spacy.tokenizer.Tokenizer(nlp.vocab)\n",
    "tokenizer = nltk.word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carry out replacement\n",
    "for index,sent in enumerate(sentences):\n",
    "    replacement_list = sentence_wise_replacements[index]    \n",
    "    for item in replacement_list[::-1]:                     \n",
    "        to_replace = item[0]                                \n",
    "        replace_with = replace_coref_with[item[1]]\n",
    "        replaced_sent = \"\"\n",
    "        words = tokenizer(sent)\n",
    "        \n",
    "        \n",
    "        for i in range(len(words)-1,to_replace[\"endIndex\"]-2,-1):\n",
    "            replaced_sent = words[i] + \" \"+ replaced_sent\n",
    "        \n",
    "        replaced_sent = replace_with + \" \" + replaced_sent\n",
    "        \n",
    "        for i in range(to_replace[\"startIndex\"]-2,-1,-1):\n",
    "            replaced_sent = words[i] + \" \"+ replaced_sent\n",
    "            \n",
    "        sentences[index] = replaced_sent\n",
    "\n",
    "result = \"\"\n",
    "for sent in sentences:\n",
    "    result += sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starwars_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relation Extraction of using Stanford OpenIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openie import StanfordOpenIE\n",
    "\n",
    "triples = []\n",
    "with StanfordOpenIE() as client:\n",
    "    for triple in client.annotate(result):\n",
    "        triples.append(triple)\n",
    "        \n",
    "triples = pd.DataFrame(triples)\n",
    "triples.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Try out MinIE and/or Allenai OpenIE-standalone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity and Triple Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_set = set(ner_dict.keys())\n",
    "entity_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_triples = []\n",
    "for row, col in triples.iterrows():\n",
    "    col['subject'] = col['subject'].strip()\n",
    "\n",
    "    if col['subject'] in entity_set:\n",
    "        added = False\n",
    "        entity2_sent = col['object']\n",
    "        for entity in entity_set:\n",
    "            if entity in entity2_sent:\n",
    "                final_triples.append((ner_dict[col['subject']], col['subject'], col['relation'], ner_dict[entity], col['object']))\n",
    "                added = True\n",
    "        if not added:\n",
    "            final_triples.append((ner_dict[col['subject']], col['subject'], col['relation'], 'O', col['object']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(final_triples, columns=['Type','Entity1','Relationship','Type', 'Entity2'])\n",
    "final_df.to_csv('starwars1_processed.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> starwars1_processed Graph Visualization: https://graphcommons.com/graphs/01252887-9a23-4b33-a06d-02729a330beb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Graph, using Stanford NER: https://graphcommons.com/graphs/1f5d76f7-69ad-4575-83dd-4c90afab7059"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triple Linking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_trf_bertbaseuncased_lg\n",
    "nlp = en_trf_bertbaseuncased_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _,col1 in final_df.iterrows():\n",
    "    head = col1['Entity2']\n",
    "    doc1 = nlp(head)\n",
    "    \n",
    "    for _,col2 in final_df.iterrows():\n",
    "        tail = col2['Entity2']\n",
    "        if head == tail:\n",
    "            continue\n",
    "            \n",
    "        doc2 = nlp(tail)\n",
    "        confidence = doc1.similarity(doc2)\n",
    "        \n",
    "        if confidence > 0.80:   # 80% seems to work pretty well\n",
    "            # Perform logic for linking\n",
    "            new_tail = tail if len(tail)<len(head) else head\n",
    "            \n",
    "            col1['Entity2'] = new_tail\n",
    "            col2['Entity2'] = new_tail\n",
    "\n",
    "            print(\"Sentence 1:\", doc1)\n",
    "            print(\"Sentence 2:\", doc2)\n",
    "            print(\"Similarity:\", confidence)\n",
    "            print(new_tail)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.drop_duplicates()\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('starwars1_linked_processed.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> https://graphcommons.com/graphs/aae63b3e-870d-4c2e-83e4-bbca9f297b42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textual Entailment with Keras_Parikh_Entailment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> https://github.com/explosion/spaCy/tree/master/examples/keras_parikh_entailment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_vectors_web_lg\n",
    "nlp = en_vectors_web_lg.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Process & Entity Linking to existing Turtle Knowledge Bases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rdflib\n",
    "graph = rdflib.Graph()\n",
    "graph.parse('../data/starwars.ttl', format='turtle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"\"\"\n",
    "    SELECT ?s ?p ?o\n",
    "    WHERE {   \n",
    "        ?s ?p ?o.\n",
    "        #FILTER(?s=\"Darth Vader\")\n",
    "    }\n",
    "    #LIMIT 10\n",
    "\"\"\"\n",
    "res = graph.query(query_str)\n",
    "\n",
    "# for s,p,o in res:\n",
    "#     print(s, '->', p, '->', o)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
